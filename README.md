
# ğŸ§  Embedding-Based Semantic Quote Search (PACE Supercomputing Project)

## ğŸ“˜ Overview
This project implements a **semantic vector search system** over a large dataset of quotes (~493,000 total), using **Hugging Face sentence embeddings** and a **FAISS** vector index for efficient similarity retrieval.  

It was executed on **Georgia Techâ€™s PACE supercomputing cluster**, leveraging GPU acceleration and batch job scheduling with **SLURM**.  

The workflow demonstrates how large-scale text embeddings can be computed, indexed, and queried on distributed compute systems â€” replicating the foundation of modern search and recommendation engines.

---

## âš™ï¸ System Architecture

### 1ï¸âƒ£ `make_index.py`
Builds the full FAISS index of embeddings.

- Loads the quotes dataset (`quotes.csv`)
- Uses the Hugging Face embedding model `google/embeddinggemma-300m`
- Embeds all quotes in GPU batches
- Stores results in:
  - `quotes.index` â†’ FAISS vector index
  - `quotes.db` â†’ SQLite database mapping IDs to quote text and authors

### 2ï¸âƒ£ `find_quote.py`
Searches the index for the closest quotes to a given query.

- Loads `quotes.index` and `quotes.db`
- Reads query sentences from `input.txt`
- Encodes each query into an embedding
- Performs top-3 FAISS nearest-neighbor search
- Writes retrieved indices to `recent_found_indices.txt`

---

## ğŸ’» HPC Setup (PACE Cluster)

### Job submission
Two SLURM batch scripts manage GPU execution:
- `job_gpu_make_index.sh` â†’ builds embeddings on GPU  
- `job_gpu_find_quote.sh` â†’ queries the index using pre-built embeddings  

Each job requests one GPU and up to 64 GB of memory:

```bash
#SBATCH --gres=gpu:H100:1
#SBATCH --mem-per-gpu=64G
#SBATCH --time=2:00:00
Authentication
Hugging Face authentication is handled securely via a stored token:
export HF_TOKEN=$(cat ~/.hf_token)
huggingface-cli login --token "$HF_TOKEN" --add-to-git-credential
Memory optimization
To prevent CUDA fragmentation:
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```
---

## ğŸ§© Models Used
Model	Purpose	Notes
google/embeddinggemma-300m	Text embedding model	Compact and GPU-efficient
sentence-transformers	Embedding pipeline framework	Provides batching & normalization
faiss-gpu	Vector database	Enables sub-millisecond similarity search

---

## ğŸ“Š Dataset
Source: CSV of ~493 k English quotes and their authors.
Each row was embedded into a 768-dimensional vector and indexed via FAISS.


ğŸ§ª Example Queries
Input (input.txt):

```bash
You can't teach an old dog new tricks.
Practice makes perfect.
Look before you leap.
Birds of a feather flock together.
```
Output (Report.out):

```bash
Processing line 4: 'Birds of a feather flock together.'
407121: 'Birds of a feather flock together.' â€” English proverb
397140: 'Birds of a feather will flock together.' â€” Minsheu
52707:  'Find your flock and fly.' â€” Jennifer Coletta
```

Final indices (recent_found_indices.txt):
```bash
414986
408814
481488
397140
```
---

## ğŸ§° Tech Stack
- Language: Python 3.10
- Libraries: sentence-transformers, faiss-gpu, huggingface_hub, sqlite3
- Hardware: NVIDIA H100 GPU (PACE Cluster)
- Tools: SLURM, Conda/venv, PACE OnDemand

---

## ğŸš€ Results
- âœ… Embedded and indexed ~493 k quotes in ~25 minutes
- âœ… Queried multiple quotes with < 0.1 s latency per query
- âœ… Learned GPU memory management, tokenized auth, and HPC job orchestration
  
---

## ğŸ§¾ Key Files
### File	Description
- `make_index.py`	â†’ Builds the FAISS index and SQLite database
- `find_quote.py`	â†’ Searches for top-k nearest quotes
- `job_gpu_make_index.sh`	â†’ SLURM job for index creation
- `job_gpu_find_quote.sh` â†’ SLURM job for query evaluation
- `requirements.txt` â†’ Required Python packages
- `recent_found_indices.txt` â†’ Output of retrieved quote indices
- `input.txt` â†’ Input quotes for semantic search

---

## ğŸ’¡ Learnings
- Managing GPU memory on large transformer models
- Using Hugging Face Hub tokens securely in HPC environments
- Efficient batching and FAISS indexing for large datasets
- SLURM scripting for multi-stage GPU workflows

